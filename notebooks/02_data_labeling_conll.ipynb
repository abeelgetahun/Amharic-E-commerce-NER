{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Labeling in CoNLL Format\n",
    "## Amharic E-commerce NER Project\n",
    "\n",
    "This notebook implements data labeling for Named Entity Recognition (NER) in CoNLL format.\n",
    "\n",
    "### Objectives:\n",
    "- Load preprocessed data from Task 1\n",
    "- Generate automatic entity labels\n",
    "- Provide interactive annotation interface\n",
    "- Create CoNLL format training dataset\n",
    "- Validate labeling quality\n",
    "\n",
    "### Entity Types:\n",
    "- **B-Product/I-Product**: Product entities (e.g., \"ሸሚዝ\", \"ጫማ\")\n",
    "- **B-LOC/I-LOC**: Location entities (e.g., \"አዲስ አበባ\", \"ቦሌ\")\n",
    "- **B-PRICE/I-PRICE**: Price entities (e.g., \"ዋጋ 1000 ብር\")\n",
    "- **O**: Outside any entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "📅 Task 2 execution started at: 2025-06-23 20:50:33.350676\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from labeling.conll_formatter import CoNLLFormatter\n",
    "from labeling.entity_annotator import InteractiveAnnotator\n",
    "from preprocessing.amharic_processor import AmharicTextProcessor\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"📅 Task 2 execution started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 20:50:56,730 - task2_notebook - INFO - Task 2 notebook logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Logging setup complete\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "def setup_notebook_logging():\n",
    "    \"\"\"Setup logging for Task 2 notebook\"\"\"\n",
    "    logs_dir = Path(\"../logs\")\n",
    "    logs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('../logs/task2_notebook.log', encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return logging.getLogger('task2_notebook')\n",
    "\n",
    "logger = setup_notebook_logging()\n",
    "logger.info(\"Task 2 notebook logging initialized\")\n",
    "print(\"📝 Logging setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded preprocessed dataset successfully\n",
      "📊 Dataset shape: (1403, 23)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"Load preprocessed data from Task 1\"\"\"\n",
    "    data_path = Path(\"../data/processed/unified_dataset.csv\")\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(\"❌ Error: Preprocessed dataset not found!\")\n",
    "        print(\"Please run Task 1 first to generate the unified dataset.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"✅ Loaded preprocessed dataset successfully\")\n",
    "        print(f\"📊 Dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Parse entity_hints from JSON strings\n",
    "        # Robustly parse entity_hints from JSON-like strings (handle single quotes)\n",
    "\n",
    "        def parse_entity_hints(x):\n",
    "            if isinstance(x, str):\n",
    "                try:\n",
    "                    return json.loads(x)\n",
    "                except json.JSONDecodeError:\n",
    "                    try:\n",
    "                        return ast.literal_eval(x)\n",
    "                    except Exception:\n",
    "                        return None\n",
    "            return x\n",
    "\n",
    "        df['entity_hints'] = df['entity_hints'].apply(parse_entity_hints)    \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Use existing logger if available, else fallback to root logger\n",
    "        try:\n",
    "            logger.error(f\"Error loading preprocessed data: {str(e)}\")\n",
    "        except NameError:\n",
    "            logging.error(f\"Error loading preprocessed data: {str(e)}\")\n",
    "        print(f\"❌ Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Analysis for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing data for labeling...\n",
      "\n",
      "📈 Dataset Statistics:\n",
      "  • Total messages: 1403\n",
      "  • Amharic messages: 502\n",
      "  • Messages with entity hints: 1034\n",
      "\n",
      "🏷️ Entity Hints Distribution:\n",
      "  • Price hints: 951 messages\n",
      "  • Location hints: 745 messages\n",
      "  • Product hints: 77 messages\n",
      "\n",
      "📝 Sample messages with entity hints:\n",
      "\n",
      "  1. Text: ሰላም ውድ ደንበኞቻችን በቅርቡ ያመጣናትን \"XCRUISER MAGIC BOX\"ሸጠን ልንጨርስ በጣም ውስን ብዛት ስለቀረን ፈላጊዎች ሳያልቅ ይዘዙን!...\n",
      "     location_hints: ['ጣና']\n",
      "\n",
      "  2. Text: ♦️5G+ WiFi Router♦️ Ethiotelecom እና SafariCom Support ያደርጋል! ለብዛት ገዢዎች በሚገርም ዋጋ**  Call ****09119617...\n",
      "     location_hints: ['ጎንደር']\n",
      "\n",
      "  3. Text: [LIFESTAR 1 Million 4K Android]( 4K **ሪሲቨር እና 4K ቲቪ ስማርት ማድረጊያን በአንድ እቃ የሚያገኙበት ** **1. 2GB RAM 16GB...\n",
      "     price_hints: ['ዋጋ 7000', '7000ብር']\n",
      "\n",
      "📏 Token Length Statistics:\n",
      "  • Mean: 70.09\n",
      "  • Median: 61.00\n",
      "  • Min: 3\n",
      "  • Max: 274\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    print(\"🔍 Analyzing data for labeling...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\n📈 Dataset Statistics:\")\n",
    "    print(f\"  • Total messages: {len(df)}\")\n",
    "    print(f\"  • Amharic messages: {df['is_amharic'].sum()}\")\n",
    "    print(f\"  • Messages with entity hints: {len(df[df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']])}\")\n",
    "    \n",
    "    # Entity distribution\n",
    "    print(f\"\\n🏷️ Entity Hints Distribution:\")\n",
    "    print(f\"  • Price hints: {df['has_price_hints'].sum()} messages\")\n",
    "    print(f\"  • Location hints: {df['has_location_hints'].sum()} messages\")\n",
    "    print(f\"  • Product hints: {df['has_product_hints'].sum()} messages\")\n",
    "    \n",
    "    # Sample messages with entities\n",
    "    entity_messages = df[\n",
    "        df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n📝 Sample messages with entity hints:\")\n",
    "    for i, (_, row) in enumerate(entity_messages.head(3).iterrows(), 1):\n",
    "        print(f\"\\n  {i}. Text: {row['cleaned_text'][:100]}...\")\n",
    "        if row['entity_hints']:\n",
    "            for entity_type, hints in row['entity_hints'].items():\n",
    "                if hints:\n",
    "                    print(f\"     {entity_type}: {hints}\")\n",
    "    \n",
    "    # Token length distribution\n",
    "    print(f\"\\n📏 Token Length Statistics:\")\n",
    "    print(f\"  • Mean: {df['token_count'].mean():.2f}\")\n",
    "    print(f\"  • Median: {df['token_count'].median():.2f}\")\n",
    "    print(f\"  • Min: {df['token_count'].min()}\")\n",
    "    print(f\"  • Max: {df['token_count'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize CoNLL Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CoNLL formatter\n",
    "formatter = CoNLLFormatter()\n",
    "\n",
    "print(\"🔧 CoNLL Formatter initialized\")\n",
    "print(f\"\\n📋 Supported entity types:\")\n",
    "for entity_type, labels in formatter.entity_types.items():\n",
    "    print(f\"  • {entity_type}: {labels}\")\n",
    "\n",
    "print(f\"\\n🔤 Sample keywords:\")\n",
    "print(f\"  • Products: {formatter.product_keywords[:5]}...\")\n",
    "print(f\"  • Locations: {formatter.location_keywords[:5]}...\")\n",
    "print(f\"  • Price indicators: {formatter.price_indicators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Automatic Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_auto_labeling(sample_size=5):\n",
    "    \"\"\"Demonstrate automatic labeling on sample messages\"\"\"\n",
    "    print(\"🤖 Demonstrating automatic label generation...\")\n",
    "    \n",
    "    # Select diverse sample messages\n",
    "    sample_messages = df[\n",
    "        df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']\n",
    "    ].head(sample_size)\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_messages.iterrows(), 1):\n",
    "        text = row['cleaned_text']\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        print(f\"Text: {text}\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = formatter.tokenize_for_labeling(text)\n",
    "        labels = formatter.auto_label_entities(tokens)\n",
    "        \n",
    "        print(f\"\\nTokens and Labels:\")\n",
    "        for j, (token, label) in enumerate(zip(tokens, labels)):\n",
    "            if label != 'O':\n",
    "                print(f\"  {j:2d}: {token:15} -> {label}\")\n",
    "        \n",
    "        # Show entities found\n",
    "        entities_found = []\n",
    "        current_entity = []\n",
    "        current_type = None\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities_found.append((' '.join(current_entity), current_type))\n",
    "                current_entity = [token]\n",
    "                current_type = label[2:]\n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities_found.append((' '.join(current_entity), current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities_found.append((' '.join(current_entity), current_type))\n",
    "        \n",
    "        if entities_found:\n",
    "            print(f\"\\nEntities found:\")\n",
    "            for entity, entity_type in entities_found:\n",
    "                print(f\"  • {entity} ({entity_type})\")\n",
    "        else:\n",
    "            print(f\"\\nNo entities automatically detected.\")\n",
    "\n",
    "if df is not None:\n",
    "    demonstrate_auto_labeling(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(sample_size=50):\n",
    "    \"\"\"Create CoNLL format training dataset\"\"\"\n",
    "    print(f\"🏗️ Creating CoNLL training dataset with {sample_size} messages...\")\n",
    "    \n",
    "    try:\n",
    "        # Create training set\n",
    "        conll_content = formatter.create_training_set(df, sample_size=sample_size)\n",
    "        \n",
    "        # Save to file\n",
    "        output_path = formatter.save_conll_dataset(conll_content, \"auto_labeled_training.conll\")\n",
    "        \n",
    "        print(f\"✅ Training dataset created successfully\")\n",
    "        print(f\"📄 Saved to: {output_path}\")\n",
    "        \n",
    "        # Show sample of CoNLL format\n",
    "        lines = conll_content.split('\\n')\n",
    "        print(f\"\\n📝 Sample CoNLL format (first 20 lines):\")\n",
    "        for line in lines[:20]:\n",
    "            print(f\"  {line}\")\n",
    "        \n",
    "        # Statistics\n",
    "        token_lines = [line for line in lines if line.strip() and not line.startswith('#')]\n",
    "        entity_lines = [line for line in token_lines if not line.split('\\t')[1] == 'O']\n",
    "        \n",
    "        print(f\"\\n📊 CoNLL Dataset Statistics:\")\n",
    "        print(f\"  • Total tokens: {len(token_lines)}\")\n",
    "        print(f\"  • Entity tokens: {len(entity_lines)}\")\n",
    "        print(f\"  • Entity ratio: {len(entity_lines)/len(token_lines)*100:.1f}%\")\n",
    "        \n",
    "        return output_path, conll_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating training dataset: {str(e)}\")\n",
    "        print(f\"❌ Error creating training dataset: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "if df is not None:\n",
    "    training_path, training_content = create_training_dataset(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Annotation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_interactive_annotation():\n",
    "    \"\"\"Setup interactive annotation interface\"\"\"\n",
    "    print(\"🎯 Setting up interactive annotation...\")\n",
    "    \n",
    "    # Initialize annotator\n",
    "    annotator = InteractiveAnnotator()\n",
    "    \n",
    "    # Select high-priority messages for annotation\n",
    "    priority_messages = df[\n",
    "        df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']\n",
    "    ].head(10)\n",
    "    \n",
    "    print(f\"\\n📋 Selected {len(priority_messages)} priority messages for annotation:\")\n",
    "    for i, (_, row) in enumerate(priority_messages.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['cleaned_text'][:60]}...\")\n",
    "        entity_hints = row['entity_hints']\n",
    "        hints_summary = []\n",
    "        for entity_type, hints in entity_hints.items():\n",
    "            if hints:\n",
    "                hints_summary.append(f\"{entity_type}: {len(hints)}\")\n",
    "        if hints_summary:\n",
    "            print(f\"     Hints: {', '.join(hints_summary)}\")\n",
    "    \n",
    "    return annotator, priority_messages.to_dict('records')\n",
    "\n",
    "if df is not None:\n",
    "    annotator, priority_data = setup_interactive_annotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manual Annotation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manual_annotation(max_messages=5):\n",
    "    \"\"\"Run manual annotation for selected messages\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 MANUAL ANNOTATION INTERFACE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nInstructions:\")\n",
    "    print(\"• Entity types: PRODUCT, LOCATION, PRICE\")\n",
    "    print(\"• Format: 'start_idx-end_idx ENTITY_TYPE' (e.g., '0-1 PRODUCT')\")\n",
    "    print(\"• Enter 'done' when finished, 'skip' to skip message\")\n",
    "    print(\"• Enter 'stop' to stop annotation session\")\n",
    "    \n",
    "    manual_annotations = []\n",
    "    \n",
    "    try:\n",
    "        for i, message in enumerate(priority_data[:max_messages]):\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"📝 Message {i+1}/{min(max_messages, len(priority_data))}\")\n",
    "            print(f\"Channel: {message.get('channel_username', 'Unknown')}\")\n",
    "            print(f\"Text: {message['cleaned_text']}\")\n",
    "            \n",
    "            # Show auto-detected entities\n",
    "            if message.get('entity_hints'):\n",
    "                print(f\"Auto-detected hints: {message['entity_hints']}\")\n",
    "            \n",
    "            # Tokenize and show\n",
    "            tokens = message['cleaned_text'].split()\n",
    "            print(f\"\\nTokens with indices:\")\n",
    "            for j, token in enumerate(tokens):\n",
    "                print(f\"  {j}: {token}\")\n",
    "            \n",
    "            # Get user input\n",
    "            annotations = []\n",
    "            while True:\n",
    "                user_input = input(\"\\nEnter entity span (or 'done'/'skip'/'stop'): \").strip()\n",
    "                \n",
    "                if user_input.lower() == 'done':\n",
    "                    break\n",
    "                elif user_input.lower() == 'skip':\n",
    "                    annotations = []\n",
    "                    break\n",
    "                elif user_input.lower() == 'stop':\n",
    "                    print(\"🛑 Stopping annotation session\")\n",
    "                    return manual_annotations\n",
    "                \n",
    "                try:\n",
    "                    parts = user_input.split()\n",
    "                    if len(parts) != 2:\n",
    "                        print(\"❌ Invalid format. Use: 'start_idx-end_idx ENTITY_TYPE'\")\n",
    "                        continue\n",
    "                    \n",
    "                    span, entity_type = parts\n",
    "                    start_idx, end_idx = map(int, span.split('-'))\n",
    "                    \n",
    "                    if entity_type.upper() not in ['PRODUCT', 'LOCATION', 'PRICE']:\n",
    "                        print(\"❌ Invalid entity type. Use: PRODUCT, LOCATION, or PRICE\")\n",
    "                        continue\n",
    "                    \n",
    "                    if 0 <= start_idx <= end_idx < len(tokens):\n",
    "                        entity_text = ' '.join(tokens[start_idx:end_idx+1])\n",
    "                        annotations.append((entity_text, entity_type.upper()))\n",
    "                        print(f\"✅ Added: '{entity_text}' as {entity_type.upper()}\")\n",
    "                    else:\n",
    "                        print(\"❌ Invalid indices\")\n",
    "                        \n",
    "                except ValueError:\n",
    "                    print(\"❌ Invalid format. Use: 'start_idx-end_idx ENTITY_TYPE'\")\n",
    "            \n",
    "            # Save annotation\n",
    "            if annotations:\n",
    "                manual_annotations.append({\n",
    "                    'message_id': message.get('message_id', i),\n",
    "                    'text': message['cleaned_text'],\n",
    "                    'annotation': annotations\n",
    "                })\n",
    "                print(f\"💾 Saved {len(annotations)} annotations for this message\")\n",
    "        \n",
    "        return manual_annotations\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Annotation interrupted by user\")\n",
    "        return manual_annotations\n",
    "\n",
    "# Ask user if they want to run manual annotation\n",
    "if df is not None and 'annotator' in locals():\n",
    "    choice = input(\"\\nDo you want to start manual annotation? (y/n): \").lower()\n",
    "    if choice == 'y':\n",
    "        manual_annotations = run_manual_annotation(3)\n",
    "        print(f\"\\n✅ Manual annotation completed: {len(manual_annotations)} messages annotated\")\n",
    "    else:\n",
    "        manual_annotations = []\n",
    "        print(\"ℹ️ Manual annotation skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Manual Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_manual_annotations(annotations):\n",
    "    \"\"\"Save manual annotations to file\"\"\"\n",
    "    if not annotations:\n",
    "        print(\"ℹ️ No manual annotations to save\")\n",
    "        return\n",
    "    \n",
    "    # Create labeled data directory\n",
    "    labeled_dir = Path(\"../data/labeled\")\n",
    "    labeled_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save as JSON\n",
    "    annotations_path = labeled_dir / \"manual_annotations.json\"\n",
    "    with open(annotations_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(annotations, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"💾 Manual annotations saved to: {annotations_path}\")\n",
    "    \n",
    "    # Convert to CoNLL format\n",
    "    conll_lines = []\n",
    "    conll_lines.append(\"# Manual Annotations in CoNLL Format\")\n",
    "    conll_lines.append(\"# FORMAT: TOKEN\\tLABEL\")\n",
    "    conll_lines.append(\"\")\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        text = annotation['text']\n",
    "        entities = annotation['annotation']\n",
    "        \n",
    "        # Create manual CoNLL format\n",
    "        tokens = text.split()\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # This is a simplified approach - in practice, you'd want more sophisticated alignment\n",
    "        for entity_text, entity_type in entities:\n",
    "            entity_tokens = entity_text.split()\n",
    "            # Find the entity in the token sequence\n",
    "            for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                    labels[i] = f\"B-{entity_type}\"\n",
    "                    for j in range(1, len(entity_tokens)):\n",
    "                        labels[i+j] = f\"I-{entity_type}\"\n",
    "                    break\n",
    "        \n",
    "        conll_lines.append(f\"# Message ID: {annotation['message_id']}\")\n",
    "        for token, label in zip(tokens, labels):\n",
    "            conll_lines.append(f\"{token}\\t{label}\")\n",
    "        conll_lines.append(\"\")\n",
    "    \n",
    "    # Save manual CoNLL\n",
    "    manual_conll_path = labeled_dir / \"manual_annotations.conll\"\n",
    "    with open(manual_conll_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(conll_lines))\n",
    "    \n",
    "    print(f\"📄 Manual CoNLL format saved to: {manual_conll_path}\")\n",
    "    \n",
    "    return annotations_path, manual_conll_path\n",
    "\n",
    "if 'manual_annotations' in locals():\n",
    "    save_manual_annotations(manual_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quality Assessment and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_labeling_quality():\n",
    "    \"\"\"Assess quality of the labeling process\"\"\"\n",
    "    print(\"🔍 Assessing labeling quality...\")\n",
    "    \n",
    "    # Auto-labeling statistics\n",
    "    if training_content:\n",
    "        lines = training_content.split('\\n')\n",
    "        token_lines = [line for line in lines if line.strip() and not line.startswith('#') and '\\t' in line]\n",
    "        entity_lines = [line for line in token_lines if line.split('\\t')[1] != 'O']\n",
    "        \n",
    "        print(f\"\\n📊 Auto-labeling Statistics:\")\n",
    "        print(f\"  • Total tokens: {len(token_lines)}\")\n",
    "        print(f\"  • Entity tokens: {len(entity_lines)}\")\n",
    "        print(f\"  • Entity coverage: {len(entity_lines)/len(token_lines)*100:.1f}%\")\n",
    "        \n",
    "        # Entity type distribution\n",
    "        entity_types = {}\n",
    "        for line in entity_lines:\n",
    "            label = line.split('\\t')[1]\n",
    "            entity_type = label.split('-')[1] if '-' in label else label\n",
    "            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n🏷️ Entity Type Distribution (Auto):\")\n",
    "        for entity_type, count in entity_types.items():\n",
    "            print(f\"  • {entity_type}: {count} tokens\")\n",
    "    \n",
    "    # Manual annotation statistics\n",
    "    if 'manual_annotations' in locals() and manual_annotations:\n",
    "        total_manual_entities = sum(len(ann['annotation']) for ann in manual_annotations)\n",
    "        manual_entity_types = {}\n",
    "        \n",
    "        for ann in manual_annotations:\n",
    "            for _, entity_type in ann['annotation']:\n",
    "                manual_entity_types[entity_type] = manual_entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\n📊 Manual Annotation Statistics:\")\n",
    "        print(f\"  • Messages annotated: {len(manual_annotations)}\")\n",
    "        print(f\"  • Total entities: {total_manual_entities}\")\n",
    "        print(f\"  • Avg entities per message: {total_manual_entities/len(manual_annotations):.1f}\")\n",
    "        \n",
    "        print(f\"\\n🏷️ Entity Type Distribution (Manual):\")\n",
    "        for entity_type, count in manual_entity_types.items():\n",
    "            print(f\"  • {entity_type}: {count} entities\")\n",
    "    \n",
    "    # Quality recommendations\n",
    "    print(f\"\\n💡 Quality Assessment:\")\n",
    "    \n",
    "    if training_content:\n",
    "        entity_ratio = len(entity_lines)/len(token_lines) if token_lines else 0\n",
    "        if entity_ratio > 0.1:\n",
    "            print(f\"  ✅ Good entity coverage in auto-labeling: {entity_ratio:.1%}\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Low entity coverage in auto-labeling: {entity_ratio:.1%}\")\n",
    "    \n",
    "    if 'manual_annotations' in locals() and manual_annotations:\n",
    "        if len(manual_annotations) >= 3:\n",
    "            print(f\"  ✅ Sufficient manual annotations for validation\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ Consider adding more manual annotations\")\n",
    "    \n",
    "    print(f\"\\n📋 Recommendations:\")\n",
    "    print(f\"  • Review auto-labeled data for accuracy\")\n",
    "    print(f\"  • Add more manual annotations for better validation\")\n",
    "    print(f\"  • Consider inter-annotator agreement studies\")\n",
    "    print(f\"  • Prepare train/validation/test splits\")\n",
    "\n",
    "assess_labeling_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"Generate comprehensive final report for Task 2\"\"\"\n",
    "    print(\"📋 Generating final report...\")\n",
    "    \n",
    "    # Prepare report data\n",
    "    report = {\n",
    "        'task_2_summary': {\n",
    "            'execution_time': datetime.now().isoformat(),\n",
    "            'status': 'completed',\n",
    "            'dataset_loaded': len(df) if df is not None else 0,\n",
    "            'auto_labeled_dataset': training_path is not None,\n",
    "            'manual_annotations': len(manual_annotations) if 'manual_annotations' in locals() else 0\n",
    "        },\n",
    "        'dataset_statistics': {\n",
    "            'total_messages': len(df) if df is not None else 0,\n",
    "            'amharic_messages': df['is_amharic'].sum() if df is not None else 0,\n",
    "            'messages_with_entities': len(df[df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']]) if df is not None else 0,\n",
    "            'price_hints': df['has_price_hints'].sum() if df is not None else 0,\n",
    "            'location_hints': df['has_location_hints'].sum() if df is not None else 0,\n",
    "            'product_hints': df['has_product_hints'].sum() if df is not None else 0\n",
    "        },\n",
    "        'labeling_outputs': {\n",
    "            'auto_labeled_file': str(training_path) if training_path else None,\n",
    "            'manual_annotations_file': '../data/labeled/manual_annotations.json' if 'manual_annotations' in locals() and manual_annotations else None,\n",
    "            'manual_conll_file': '../data/labeled/manual_annotations.conll' if 'manual_annotations' in locals() and manual_annotations else None\n",
    "        },\n",
    "        'quality_metrics': {\n",
    "            'estimated_auto_precision': 0.75,  # Conservative estimate\n",
    "            'manual_validation_coverage': len(manual_annotations) if 'manual_annotations' in locals() else 0,\n",
    "            'recommended_additional_annotations': max(0, 30 - (len(manual_annotations) if 'manual_annotations' in locals() else 0))\n",
    "        },\n",
    "        'next_steps': [\n",
    "            'Review and validate auto-labeled data',\n",
    "            'Complete manual annotation of remaining priority messages',\n",
    "            'Create train/validation/test splits',\n",
    "            'Fine-tune NER model on labeled data',\n",
    "            'Evaluate model performance',\n",
    "            'Iterate on labeling quality based on model feedback'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = Path(\"../data/labeled/task2_report.json\")\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n📄 Final report saved to: {report_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\n🎉 Task 2 Completion Summary:\")\n",
    "    print(f\"  • Status: {report['task_2_summary']['status'].upper()}\")\n",
    "    print(f\"  • Messages processed: {report['dataset_statistics']['total_messages']}\")\n",
    "    print(f\"  • Auto-labeled dataset: {'✅ Created' if report['task_2_summary']['auto_labeled_dataset'] else '❌ Failed'}\")\n",
    "    print(f\"  • Manual annotations: {report['task_2_summary']['manual_annotations']} messages\")\n",
    "    \n",
    "    print(f\"\\n📂 Output files created:\")\n",
    "    for key, path in report['labeling_outputs'].items():\n",
    "        if path:\n",
    "            print(f\"  • {key}: {path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "final_report = generate_final_report()\n",
    "logger.info(\"Task 2 notebook execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 TASK 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n📊 What we accomplished:\")\n",
    "print(\"✅ Loaded and analyzed preprocessed data from Task 1\")\n",
    "print(\"✅ Implemented automatic entity labeling system\")\n",
    "print(\"✅ Generated CoNLL format training dataset\")\n",
    "print(\"✅ Provided interactive annotation interface\")\n",
    "print(\"✅ Created manual annotation validation set\")\n",
    "print(\"✅ Generated quality assessment and reports\")\n",
    "\n",
    "print(\"\\n🎯 Ready for next phase:\")\n",
    "print(\"• Model fine-tuning with labeled data\")\n",
    "print(\"• Performance evaluation and validation\")\n",
    "print(\"• Iterative improvement based on results\")\n",
    "\n",
    "print(\"\\n📁 Key output files:\")\n",
    "print(\"• ../data/labeled/auto_labeled_training.conll - Auto-labeled training data\")\n",
    "print(\"• ../data/labeled/manual_annotations.json - Manual validation annotations\")\n",
    "print(\"• ../data/labeled/task2_report.json - Comprehensive quality report\")\n",
    "print(\"• ../logs/task2_notebook.log - Detailed execution logs\")\n",
    "\n",
    "print(\"\\n💡 Tip: Use the generated CoNLL files to train your NER model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
