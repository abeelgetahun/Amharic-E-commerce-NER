{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Data Labeling in CoNLL Format\n",
    "## Amharic E-commerce NER Project\n",
    "\n",
    "This notebook implements data labeling for Named Entity Recognition (NER) in CoNLL format.\n",
    "\n",
    "### Objectives:\n",
    "- Load preprocessed data from Task 1\n",
    "- Generate automatic entity labels\n",
    "- Provide interactive annotation interface\n",
    "- Create CoNLL format training dataset\n",
    "- Validate labeling quality\n",
    "\n",
    "### Entity Types:\n",
    "- **B-Product/I-Product**: Product entities (e.g., \"·à∏·àö·ãù\", \"·å´·àõ\")\n",
    "- **B-LOC/I-LOC**: Location entities (e.g., \"·ä†·ã≤·àµ ·ä†·â†·â£\", \"·â¶·àå\")\n",
    "- **B-PRICE/I-PRICE**: Price entities (e.g., \"·ãã·åã 1000 ·â•·à≠\")\n",
    "- **O**: Outside any entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "üìÖ Task 2 execution started at: 2025-06-23 20:50:33.350676\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from labeling.conll_formatter import CoNLLFormatter\n",
    "from labeling.entity_annotator import InteractiveAnnotator\n",
    "from preprocessing.amharic_processor import AmharicTextProcessor\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÖ Task 2 execution started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 20:50:56,730 - task2_notebook - INFO - Task 2 notebook logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Logging setup complete\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "def setup_notebook_logging():\n",
    "    \"\"\"Setup logging for Task 2 notebook\"\"\"\n",
    "    logs_dir = Path(\"../logs\")\n",
    "    logs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('../logs/task2_notebook.log', encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return logging.getLogger('task2_notebook')\n",
    "\n",
    "logger = setup_notebook_logging()\n",
    "logger.info(\"Task 2 notebook logging initialized\")\n",
    "print(\"üìù Logging setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded preprocessed dataset successfully\n",
      "üìä Dataset shape: (1403, 23)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "def load_preprocessed_data():\n",
    "    \"\"\"Load preprocessed data from Task 1\"\"\"\n",
    "    data_path = Path(\"../data/processed/unified_dataset.csv\")\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(\"‚ùå Error: Preprocessed dataset not found!\")\n",
    "        print(\"Please run Task 1 first to generate the unified dataset.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"‚úÖ Loaded preprocessed dataset successfully\")\n",
    "        print(f\"üìä Dataset shape: {df.shape}\")\n",
    "        \n",
    "        # Parse entity_hints from JSON strings\n",
    "        # Robustly parse entity_hints from JSON-like strings (handle single quotes)\n",
    "\n",
    "        def parse_entity_hints(x):\n",
    "            if isinstance(x, str):\n",
    "                try:\n",
    "                    return json.loads(x)\n",
    "                except json.JSONDecodeError:\n",
    "                    try:\n",
    "                        return ast.literal_eval(x)\n",
    "                    except Exception:\n",
    "                        return None\n",
    "            return x\n",
    "\n",
    "        df['entity_hints'] = df['entity_hints'].apply(parse_entity_hints)    \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Use existing logger if available, else fallback to root logger\n",
    "        try:\n",
    "            logger.error(f\"Error loading preprocessed data: {str(e)}\")\n",
    "        except NameError:\n",
    "            logging.error(f\"Error loading preprocessed data: {str(e)}\")\n",
    "        print(f\"‚ùå Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Analysis for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing data for labeling...\n",
      "\n",
      "üìà Dataset Statistics:\n",
      "  ‚Ä¢ Total messages: 1403\n",
      "  ‚Ä¢ Amharic messages: 502\n",
      "  ‚Ä¢ Messages with entity hints: 1034\n",
      "\n",
      "üè∑Ô∏è Entity Hints Distribution:\n",
      "  ‚Ä¢ Price hints: 951 messages\n",
      "  ‚Ä¢ Location hints: 745 messages\n",
      "  ‚Ä¢ Product hints: 77 messages\n",
      "\n",
      "üìù Sample messages with entity hints:\n",
      "\n",
      "  1. Text: ·à∞·àã·àù ·ãç·ãµ ·ã∞·äï·â†·äû·âª·âΩ·äï ·â†·âÖ·à≠·â° ·ã´·àò·å£·äì·âµ·äï \"XCRUISER MAGIC BOX\"·à∏·å†·äï ·àç·äï·å®·à≠·àµ ·â†·å£·àù ·ãç·àµ·äï ·â•·ãõ·âµ ·àµ·àà·âÄ·à®·äï ·çà·àã·åä·ãé·âΩ ·à≥·ã´·àç·âÖ ·ã≠·ãò·ãô·äï!...\n",
      "     location_hints: ['·å£·äì']\n",
      "\n",
      "  2. Text: ‚ô¶Ô∏è5G+ WiFi Router‚ô¶Ô∏è Ethiotelecom ·ä•·äì SafariCom Support ·ã´·ã∞·à≠·åã·àç! ·àà·â•·ãõ·âµ ·åà·ã¢·ãé·âΩ ·â†·àö·åà·à≠·àù ·ãã·åã**  Call ****09119617...\n",
      "     location_hints: ['·åé·äï·ã∞·à≠']\n",
      "\n",
      "  3. Text: [LIFESTAR 1 Million 4K Android]( 4K **·à™·à≤·â®·à≠ ·ä•·äì 4K ·â≤·â™ ·àµ·àõ·à≠·âµ ·àõ·ãµ·à®·åä·ã´·äï ·â†·ä†·äï·ãµ ·ä•·âÉ ·ã®·àö·ã´·åà·äô·â†·âµ ** **1. 2GB RAM 16GB...\n",
      "     price_hints: ['·ãã·åã 7000', '7000·â•·à≠']\n",
      "\n",
      "üìè Token Length Statistics:\n",
      "  ‚Ä¢ Mean: 70.09\n",
      "  ‚Ä¢ Median: 61.00\n",
      "  ‚Ä¢ Min: 3\n",
      "  ‚Ä¢ Max: 274\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    print(\"üîç Analyzing data for labeling...\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìà Dataset Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total messages: {len(df)}\")\n",
    "    print(f\"  ‚Ä¢ Amharic messages: {df['is_amharic'].sum()}\")\n",
    "    print(f\"  ‚Ä¢ Messages with entity hints: {len(df[df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']])}\")\n",
    "    \n",
    "    # Entity distribution\n",
    "    print(f\"\\nüè∑Ô∏è Entity Hints Distribution:\")\n",
    "    print(f\"  ‚Ä¢ Price hints: {df['has_price_hints'].sum()} messages\")\n",
    "    print(f\"  ‚Ä¢ Location hints: {df['has_location_hints'].sum()} messages\")\n",
    "    print(f\"  ‚Ä¢ Product hints: {df['has_product_hints'].sum()} messages\")\n",
    "    \n",
    "    # Sample messages with entities\n",
    "    entity_messages = df[\n",
    "        df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìù Sample messages with entity hints:\")\n",
    "    for i, (_, row) in enumerate(entity_messages.head(3).iterrows(), 1):\n",
    "        print(f\"\\n  {i}. Text: {row['cleaned_text'][:100]}...\")\n",
    "        if row['entity_hints']:\n",
    "            for entity_type, hints in row['entity_hints'].items():\n",
    "                if hints:\n",
    "                    print(f\"     {entity_type}: {hints}\")\n",
    "    \n",
    "    # Token length distribution\n",
    "    print(f\"\\nüìè Token Length Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Mean: {df['token_count'].mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Median: {df['token_count'].median():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Min: {df['token_count'].min()}\")\n",
    "    print(f\"  ‚Ä¢ Max: {df['token_count'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize CoNLL Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CoNLL formatter\n",
    "formatter = CoNLLFormatter()\n",
    "\n",
    "print(\"üîß CoNLL Formatter initialized\")\n",
    "print(f\"\\nüìã Supported entity types:\")\n",
    "for entity_type, labels in formatter.entity_types.items():\n",
    "    print(f\"  ‚Ä¢ {entity_type}: {labels}\")\n",
    "\n",
    "print(f\"\\nüî§ Sample keywords:\")\n",
    "print(f\"  ‚Ä¢ Products: {formatter.product_keywords[:5]}...\")\n",
    "print(f\"  ‚Ä¢ Locations: {formatter.location_keywords[:5]}...\")\n",
    "print(f\"  ‚Ä¢ Price indicators: {formatter.price_indicators}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Automatic Label Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_auto_labeling(sample_size=5):\n",
    "    \"\"\"Demonstrate automatic labeling on sample messages\"\"\"\n",
    "    print(\"ü§ñ Demonstrating automatic label generation...\")\n",
    "    \n",
    "    # Select diverse sample messages\n",
    "    sample_messages = df[\n",
    "        df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']\n",
    "    ].head(sample_size)\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_messages.iterrows(), 1):\n",
    "        text = row['cleaned_text']\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        print(f\"Text: {text}\")\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = formatter.tokenize_for_labeling(text)\n",
    "        labels = formatter.auto_label_entities(tokens)\n",
    "        \n",
    "        print(f\"\\nTokens and Labels:\")\n",
    "        for j, (token, label) in enumerate(zip(tokens, labels)):\n",
    "            if label != 'O':\n",
    "                print(f\"  {j:2d}: {token:15} -> {label}\")\n",
    "        \n",
    "        # Show entities found\n",
    "        entities_found = []\n",
    "        current_entity = []\n",
    "        current_type = None\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities_found.append((' '.join(current_entity), current_type))\n",
    "                current_entity = [token]\n",
    "                current_type = label[2:]\n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities_found.append((' '.join(current_entity), current_type))\n",
    "                current_entity = []\n",
    "                current_type = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities_found.append((' '.join(current_entity), current_type))\n",
    "        \n",
    "        if entities_found:\n",
    "            print(f\"\\nEntities found:\")\n",
    "            for entity, entity_type in entities_found:\n",
    "                print(f\"  ‚Ä¢ {entity} ({entity_type})\")\n",
    "        else:\n",
    "            print(f\"\\nNo entities automatically detected.\")\n",
    "\n",
    "if df is not None:\n",
    "    demonstrate_auto_labeling(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(sample_size=50):\n",
    "    \"\"\"Create CoNLL format training dataset\"\"\"\n",
    "    print(f\"üèóÔ∏è Creating CoNLL training dataset with {sample_size} messages...\")\n",
    "    \n",
    "    try:\n",
    "        # Create training set\n",
    "        conll_content = formatter.create_training_set(df, sample_size=sample_size)\n",
    "        \n",
    "        # Save to file\n",
    "        output_path = formatter.save_conll_dataset(conll_content, \"auto_labeled_training.conll\")\n",
    "        \n",
    "        print(f\"‚úÖ Training dataset created successfully\")\n",
    "        print(f\"üìÑ Saved to: {output_path}\")\n",
    "        \n",
    "        # Show sample of CoNLL format\n",
    "        lines = conll_content.split('\\n')\n",
    "        print(f\"\\nüìù Sample CoNLL format (first 20 lines):\")\n",
    "        for line in lines[:20]:\n",
    "            print(f\"  {line}\")\n",
    "        \n",
    "        # Statistics\n",
    "        token_lines = [line for line in lines if line.strip() and not line.startswith('#')]\n",
    "        entity_lines = [line for line in token_lines if not line.split('\\t')[1] == 'O']\n",
    "        \n",
    "        print(f\"\\nüìä CoNLL Dataset Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total tokens: {len(token_lines)}\")\n",
    "        print(f\"  ‚Ä¢ Entity tokens: {len(entity_lines)}\")\n",
    "        print(f\"  ‚Ä¢ Entity ratio: {len(entity_lines)/len(token_lines)*100:.1f}%\")\n",
    "        \n",
    "        return output_path, conll_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating training dataset: {str(e)}\")\n",
    "        print(f\"‚ùå Error creating training dataset: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "if df is not None:\n",
    "    training_path, training_content = create_training_dataset(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Annotation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_interactive_annotation():\n",
    "    \"\"\"Setup interactive annotation interface\"\"\"\n",
    "    print(\"üéØ Setting up interactive annotation...\")\n",
    "    \n",
    "    # Initialize annotator\n",
    "    annotator = InteractiveAnnotator()\n",
    "    \n",
    "    # Select high-priority messages for annotation\n",
    "    priority_messages = df[\n",
    "        df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']\n",
    "    ].head(10)\n",
    "    \n",
    "    print(f\"\\nüìã Selected {len(priority_messages)} priority messages for annotation:\")\n",
    "    for i, (_, row) in enumerate(priority_messages.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['cleaned_text'][:60]}...\")\n",
    "        entity_hints = row['entity_hints']\n",
    "        hints_summary = []\n",
    "        for entity_type, hints in entity_hints.items():\n",
    "            if hints:\n",
    "                hints_summary.append(f\"{entity_type}: {len(hints)}\")\n",
    "        if hints_summary:\n",
    "            print(f\"     Hints: {', '.join(hints_summary)}\")\n",
    "    \n",
    "    return annotator, priority_messages.to_dict('records')\n",
    "\n",
    "if df is not None:\n",
    "    annotator, priority_data = setup_interactive_annotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Manual Annotation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manual_annotation(max_messages=5):\n",
    "    \"\"\"Run manual annotation for selected messages\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ MANUAL ANNOTATION INTERFACE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nInstructions:\")\n",
    "    print(\"‚Ä¢ Entity types: PRODUCT, LOCATION, PRICE\")\n",
    "    print(\"‚Ä¢ Format: 'start_idx-end_idx ENTITY_TYPE' (e.g., '0-1 PRODUCT')\")\n",
    "    print(\"‚Ä¢ Enter 'done' when finished, 'skip' to skip message\")\n",
    "    print(\"‚Ä¢ Enter 'stop' to stop annotation session\")\n",
    "    \n",
    "    manual_annotations = []\n",
    "    \n",
    "    try:\n",
    "        for i, message in enumerate(priority_data[:max_messages]):\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"üìù Message {i+1}/{min(max_messages, len(priority_data))}\")\n",
    "            print(f\"Channel: {message.get('channel_username', 'Unknown')}\")\n",
    "            print(f\"Text: {message['cleaned_text']}\")\n",
    "            \n",
    "            # Show auto-detected entities\n",
    "            if message.get('entity_hints'):\n",
    "                print(f\"Auto-detected hints: {message['entity_hints']}\")\n",
    "            \n",
    "            # Tokenize and show\n",
    "            tokens = message['cleaned_text'].split()\n",
    "            print(f\"\\nTokens with indices:\")\n",
    "            for j, token in enumerate(tokens):\n",
    "                print(f\"  {j}: {token}\")\n",
    "            \n",
    "            # Get user input\n",
    "            annotations = []\n",
    "            while True:\n",
    "                user_input = input(\"\\nEnter entity span (or 'done'/'skip'/'stop'): \").strip()\n",
    "                \n",
    "                if user_input.lower() == 'done':\n",
    "                    break\n",
    "                elif user_input.lower() == 'skip':\n",
    "                    annotations = []\n",
    "                    break\n",
    "                elif user_input.lower() == 'stop':\n",
    "                    print(\"üõë Stopping annotation session\")\n",
    "                    return manual_annotations\n",
    "                \n",
    "                try:\n",
    "                    parts = user_input.split()\n",
    "                    if len(parts) != 2:\n",
    "                        print(\"‚ùå Invalid format. Use: 'start_idx-end_idx ENTITY_TYPE'\")\n",
    "                        continue\n",
    "                    \n",
    "                    span, entity_type = parts\n",
    "                    start_idx, end_idx = map(int, span.split('-'))\n",
    "                    \n",
    "                    if entity_type.upper() not in ['PRODUCT', 'LOCATION', 'PRICE']:\n",
    "                        print(\"‚ùå Invalid entity type. Use: PRODUCT, LOCATION, or PRICE\")\n",
    "                        continue\n",
    "                    \n",
    "                    if 0 <= start_idx <= end_idx < len(tokens):\n",
    "                        entity_text = ' '.join(tokens[start_idx:end_idx+1])\n",
    "                        annotations.append((entity_text, entity_type.upper()))\n",
    "                        print(f\"‚úÖ Added: '{entity_text}' as {entity_type.upper()}\")\n",
    "                    else:\n",
    "                        print(\"‚ùå Invalid indices\")\n",
    "                        \n",
    "                except ValueError:\n",
    "                    print(\"‚ùå Invalid format. Use: 'start_idx-end_idx ENTITY_TYPE'\")\n",
    "            \n",
    "            # Save annotation\n",
    "            if annotations:\n",
    "                manual_annotations.append({\n",
    "                    'message_id': message.get('message_id', i),\n",
    "                    'text': message['cleaned_text'],\n",
    "                    'annotation': annotations\n",
    "                })\n",
    "                print(f\"üíæ Saved {len(annotations)} annotations for this message\")\n",
    "        \n",
    "        return manual_annotations\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüõë Annotation interrupted by user\")\n",
    "        return manual_annotations\n",
    "\n",
    "# Ask user if they want to run manual annotation\n",
    "if df is not None and 'annotator' in locals():\n",
    "    choice = input(\"\\nDo you want to start manual annotation? (y/n): \").lower()\n",
    "    if choice == 'y':\n",
    "        manual_annotations = run_manual_annotation(3)\n",
    "        print(f\"\\n‚úÖ Manual annotation completed: {len(manual_annotations)} messages annotated\")\n",
    "    else:\n",
    "        manual_annotations = []\n",
    "        print(\"‚ÑπÔ∏è Manual annotation skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Manual Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_manual_annotations(annotations):\n",
    "    \"\"\"Save manual annotations to file\"\"\"\n",
    "    if not annotations:\n",
    "        print(\"‚ÑπÔ∏è No manual annotations to save\")\n",
    "        return\n",
    "    \n",
    "    # Create labeled data directory\n",
    "    labeled_dir = Path(\"../data/labeled\")\n",
    "    labeled_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save as JSON\n",
    "    annotations_path = labeled_dir / \"manual_annotations.json\"\n",
    "    with open(annotations_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(annotations, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Manual annotations saved to: {annotations_path}\")\n",
    "    \n",
    "    # Convert to CoNLL format\n",
    "    conll_lines = []\n",
    "    conll_lines.append(\"# Manual Annotations in CoNLL Format\")\n",
    "    conll_lines.append(\"# FORMAT: TOKEN\\tLABEL\")\n",
    "    conll_lines.append(\"\")\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        text = annotation['text']\n",
    "        entities = annotation['annotation']\n",
    "        \n",
    "        # Create manual CoNLL format\n",
    "        tokens = text.split()\n",
    "        labels = ['O'] * len(tokens)\n",
    "        \n",
    "        # This is a simplified approach - in practice, you'd want more sophisticated alignment\n",
    "        for entity_text, entity_type in entities:\n",
    "            entity_tokens = entity_text.split()\n",
    "            # Find the entity in the token sequence\n",
    "            for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "                if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                    labels[i] = f\"B-{entity_type}\"\n",
    "                    for j in range(1, len(entity_tokens)):\n",
    "                        labels[i+j] = f\"I-{entity_type}\"\n",
    "                    break\n",
    "        \n",
    "        conll_lines.append(f\"# Message ID: {annotation['message_id']}\")\n",
    "        for token, label in zip(tokens, labels):\n",
    "            conll_lines.append(f\"{token}\\t{label}\")\n",
    "        conll_lines.append(\"\")\n",
    "    \n",
    "    # Save manual CoNLL\n",
    "    manual_conll_path = labeled_dir / \"manual_annotations.conll\"\n",
    "    with open(manual_conll_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(conll_lines))\n",
    "    \n",
    "    print(f\"üìÑ Manual CoNLL format saved to: {manual_conll_path}\")\n",
    "    \n",
    "    return annotations_path, manual_conll_path\n",
    "\n",
    "if 'manual_annotations' in locals():\n",
    "    save_manual_annotations(manual_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quality Assessment and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_labeling_quality():\n",
    "    \"\"\"Assess quality of the labeling process\"\"\"\n",
    "    print(\"üîç Assessing labeling quality...\")\n",
    "    \n",
    "    # Auto-labeling statistics\n",
    "    if training_content:\n",
    "        lines = training_content.split('\\n')\n",
    "        token_lines = [line for line in lines if line.strip() and not line.startswith('#') and '\\t' in line]\n",
    "        entity_lines = [line for line in token_lines if line.split('\\t')[1] != 'O']\n",
    "        \n",
    "        print(f\"\\nüìä Auto-labeling Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total tokens: {len(token_lines)}\")\n",
    "        print(f\"  ‚Ä¢ Entity tokens: {len(entity_lines)}\")\n",
    "        print(f\"  ‚Ä¢ Entity coverage: {len(entity_lines)/len(token_lines)*100:.1f}%\")\n",
    "        \n",
    "        # Entity type distribution\n",
    "        entity_types = {}\n",
    "        for line in entity_lines:\n",
    "            label = line.split('\\t')[1]\n",
    "            entity_type = label.split('-')[1] if '-' in label else label\n",
    "            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è Entity Type Distribution (Auto):\")\n",
    "        for entity_type, count in entity_types.items():\n",
    "            print(f\"  ‚Ä¢ {entity_type}: {count} tokens\")\n",
    "    \n",
    "    # Manual annotation statistics\n",
    "    if 'manual_annotations' in locals() and manual_annotations:\n",
    "        total_manual_entities = sum(len(ann['annotation']) for ann in manual_annotations)\n",
    "        manual_entity_types = {}\n",
    "        \n",
    "        for ann in manual_annotations:\n",
    "            for _, entity_type in ann['annotation']:\n",
    "                manual_entity_types[entity_type] = manual_entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        print(f\"\\nüìä Manual Annotation Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Messages annotated: {len(manual_annotations)}\")\n",
    "        print(f\"  ‚Ä¢ Total entities: {total_manual_entities}\")\n",
    "        print(f\"  ‚Ä¢ Avg entities per message: {total_manual_entities/len(manual_annotations):.1f}\")\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è Entity Type Distribution (Manual):\")\n",
    "        for entity_type, count in manual_entity_types.items():\n",
    "            print(f\"  ‚Ä¢ {entity_type}: {count} entities\")\n",
    "    \n",
    "    # Quality recommendations\n",
    "    print(f\"\\nüí° Quality Assessment:\")\n",
    "    \n",
    "    if training_content:\n",
    "        entity_ratio = len(entity_lines)/len(token_lines) if token_lines else 0\n",
    "        if entity_ratio > 0.1:\n",
    "            print(f\"  ‚úÖ Good entity coverage in auto-labeling: {entity_ratio:.1%}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Low entity coverage in auto-labeling: {entity_ratio:.1%}\")\n",
    "    \n",
    "    if 'manual_annotations' in locals() and manual_annotations:\n",
    "        if len(manual_annotations) >= 3:\n",
    "            print(f\"  ‚úÖ Sufficient manual annotations for validation\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Consider adding more manual annotations\")\n",
    "    \n",
    "    print(f\"\\nüìã Recommendations:\")\n",
    "    print(f\"  ‚Ä¢ Review auto-labeled data for accuracy\")\n",
    "    print(f\"  ‚Ä¢ Add more manual annotations for better validation\")\n",
    "    print(f\"  ‚Ä¢ Consider inter-annotator agreement studies\")\n",
    "    print(f\"  ‚Ä¢ Prepare train/validation/test splits\")\n",
    "\n",
    "assess_labeling_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"Generate comprehensive final report for Task 2\"\"\"\n",
    "    print(\"üìã Generating final report...\")\n",
    "    \n",
    "    # Prepare report data\n",
    "    report = {\n",
    "        'task_2_summary': {\n",
    "            'execution_time': datetime.now().isoformat(),\n",
    "            'status': 'completed',\n",
    "            'dataset_loaded': len(df) if df is not None else 0,\n",
    "            'auto_labeled_dataset': training_path is not None,\n",
    "            'manual_annotations': len(manual_annotations) if 'manual_annotations' in locals() else 0\n",
    "        },\n",
    "        'dataset_statistics': {\n",
    "            'total_messages': len(df) if df is not None else 0,\n",
    "            'amharic_messages': df['is_amharic'].sum() if df is not None else 0,\n",
    "            'messages_with_entities': len(df[df['has_price_hints'] | df['has_location_hints'] | df['has_product_hints']]) if df is not None else 0,\n",
    "            'price_hints': df['has_price_hints'].sum() if df is not None else 0,\n",
    "            'location_hints': df['has_location_hints'].sum() if df is not None else 0,\n",
    "            'product_hints': df['has_product_hints'].sum() if df is not None else 0\n",
    "        },\n",
    "        'labeling_outputs': {\n",
    "            'auto_labeled_file': str(training_path) if training_path else None,\n",
    "            'manual_annotations_file': '../data/labeled/manual_annotations.json' if 'manual_annotations' in locals() and manual_annotations else None,\n",
    "            'manual_conll_file': '../data/labeled/manual_annotations.conll' if 'manual_annotations' in locals() and manual_annotations else None\n",
    "        },\n",
    "        'quality_metrics': {\n",
    "            'estimated_auto_precision': 0.75,  # Conservative estimate\n",
    "            'manual_validation_coverage': len(manual_annotations) if 'manual_annotations' in locals() else 0,\n",
    "            'recommended_additional_annotations': max(0, 30 - (len(manual_annotations) if 'manual_annotations' in locals() else 0))\n",
    "        },\n",
    "        'next_steps': [\n",
    "            'Review and validate auto-labeled data',\n",
    "            'Complete manual annotation of remaining priority messages',\n",
    "            'Create train/validation/test splits',\n",
    "            'Fine-tune NER model on labeled data',\n",
    "            'Evaluate model performance',\n",
    "            'Iterate on labeling quality based on model feedback'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = Path(\"../data/labeled/task2_report.json\")\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nüìÑ Final report saved to: {report_path}\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"\\nüéâ Task 2 Completion Summary:\")\n",
    "    print(f\"  ‚Ä¢ Status: {report['task_2_summary']['status'].upper()}\")\n",
    "    print(f\"  ‚Ä¢ Messages processed: {report['dataset_statistics']['total_messages']}\")\n",
    "    print(f\"  ‚Ä¢ Auto-labeled dataset: {'‚úÖ Created' if report['task_2_summary']['auto_labeled_dataset'] else '‚ùå Failed'}\")\n",
    "    print(f\"  ‚Ä¢ Manual annotations: {report['task_2_summary']['manual_annotations']} messages\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Output files created:\")\n",
    "    for key, path in report['labeling_outputs'].items():\n",
    "        if path:\n",
    "            print(f\"  ‚Ä¢ {key}: {path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "final_report = generate_final_report()\n",
    "logger.info(\"Task 2 notebook execution completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TASK 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä What we accomplished:\")\n",
    "print(\"‚úÖ Loaded and analyzed preprocessed data from Task 1\")\n",
    "print(\"‚úÖ Implemented automatic entity labeling system\")\n",
    "print(\"‚úÖ Generated CoNLL format training dataset\")\n",
    "print(\"‚úÖ Provided interactive annotation interface\")\n",
    "print(\"‚úÖ Created manual annotation validation set\")\n",
    "print(\"‚úÖ Generated quality assessment and reports\")\n",
    "\n",
    "print(\"\\nüéØ Ready for next phase:\")\n",
    "print(\"‚Ä¢ Model fine-tuning with labeled data\")\n",
    "print(\"‚Ä¢ Performance evaluation and validation\")\n",
    "print(\"‚Ä¢ Iterative improvement based on results\")\n",
    "\n",
    "print(\"\\nüìÅ Key output files:\")\n",
    "print(\"‚Ä¢ ../data/labeled/auto_labeled_training.conll - Auto-labeled training data\")\n",
    "print(\"‚Ä¢ ../data/labeled/manual_annotations.json - Manual validation annotations\")\n",
    "print(\"‚Ä¢ ../data/labeled/task2_report.json - Comprehensive quality report\")\n",
    "print(\"‚Ä¢ ../logs/task2_notebook.log - Detailed execution logs\")\n",
    "\n",
    "print(\"\\nüí° Tip: Use the generated CoNLL files to train your NER model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
