{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "task4-header",
      "metadata": {},
      "source": [
        "# Task 4: Model Comparison & Selection\n",
        "\n",
        "This notebook performs a comprehensive comparison of multiple transformer models for Amharic Named Entity Recognition (NER). We'll evaluate three different multilingual models and select the best one based on various performance metrics.\n",
        "\n",
        "## Models to Compare:\n",
        "- **XLM-RoBERTa-base**: Cross-lingual language model with robust multilingual support\n",
        "- **BERT-base-multilingual-cased**: Google's multilingual BERT model\n",
        "- **DistilBERT-base-multilingual-cased**: Distilled version of multilingual BERT (faster, smaller)\n",
        "\n",
        "## Evaluation Criteria:\n",
        "- **Accuracy Metrics**: F1-score, Precision, Recall, Accuracy\n",
        "- **Efficiency Metrics**: Training time, Inference time, Model size\n",
        "- **Business Considerations**: Resource requirements, deployment feasibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-section",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "Setting up the necessary libraries for model comparison, visualization, and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-config",
      "metadata": {},
      "source": [
        "## 2. Model Configuration\n",
        "\n",
        "Define the models we want to compare and initialize the results storage structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model-setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of models to compare\n",
        "models_to_compare = [\n",
        "    \"xlm-roberta-base\",\n",
        "    \"bert-base-multilingual-cased\", \n",
        "    \"distilbert-base-multilingual-cased\"\n",
        "]\n",
        "\n",
        "# Dictionary to store results\n",
        "model_results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training-function",
      "metadata": {},
      "source": [
        "## 3. Model Training and Evaluation Function\n",
        "\n",
        "This comprehensive function handles the complete training and evaluation pipeline for each model. It:\n",
        "\n",
        "- Loads the specified model and tokenizer\n",
        "- Tokenizes and prepares datasets\n",
        "- Configures training parameters\n",
        "- Trains the model\n",
        "- Evaluates performance on test set\n",
        "- Measures timing and resource metrics\n",
        "- Saves the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training-function-def",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model_name, train_dataset, val_dataset, test_dataset, \n",
        "                           tokenizer_name=None, epochs=3):\n",
        "    \"\"\"Train and evaluate a specific model\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training model: {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Use same tokenizer name as model if not specified\n",
        "    if tokenizer_name is None:\n",
        "        tokenizer_name = model_name\n",
        "    \n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name, \n",
        "        num_labels=len(label_list),\n",
        "        id2label=id_to_label,\n",
        "        label2id=label_to_id\n",
        "    )\n",
        "    \n",
        "    # Tokenize datasets\n",
        "    start_time = time()\n",
        "    train_tokenized = train_dataset.map(\n",
        "        lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id),\n",
        "        batched=True\n",
        "    )\n",
        "    val_tokenized = val_dataset.map(\n",
        "        lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id),\n",
        "        batched=True\n",
        "    )\n",
        "    test_tokenized = test_dataset.map(\n",
        "        lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id),\n",
        "        batched=True\n",
        "    )\n",
        "    tokenization_time = time() - start_time\n",
        "    \n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForTokenClassification(\n",
        "        tokenizer=tokenizer, \n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_{model_name.replace('/', '_')}\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=f'./logs_{model_name.replace(\"/\", \"_\")}',\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        save_total_limit=1,\n",
        "        report_to=None,\n",
        "        dataloader_pin_memory=False,\n",
        "    )\n",
        "    \n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized,\n",
        "        eval_dataset=val_tokenized,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    start_time = time()\n",
        "    trainer.train()\n",
        "    training_time = time() - start_time\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    start_time = time()\n",
        "    test_results = trainer.evaluate(test_tokenized)\n",
        "    inference_time = time() - start_time\n",
        "    \n",
        "    # Calculate model size (approximate)\n",
        "    model_size = sum(p.numel() for p in model.parameters()) / 1e6  # in millions\n",
        "    \n",
        "    # Store results\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'test_f1': test_results['eval_f1'],\n",
        "        'test_precision': test_results['eval_precision'],\n",
        "        'test_recall': test_results['eval_recall'],\n",
        "        'test_accuracy': test_results['eval_accuracy'],\n",
        "        'training_time': training_time,\n",
        "        'inference_time': inference_time,\n",
        "        'tokenization_time': tokenization_time,\n",
        "        'model_size_millions': model_size,\n",
        "        'test_loss': test_results['eval_loss']\n",
        "    }\n",
        "    \n",
        "    # Save model\n",
        "    model_save_path = f\"./{model_name.replace('/', '_')}_amharic_ner\"\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    \n",
        "    return results, model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-training",
      "metadata": {},
      "source": [
        "## 4. Execute Model Training and Evaluation\n",
        "\n",
        "Train and evaluate each model in our comparison list. This process will:\n",
        "\n",
        "- Train each model for the specified number of epochs\n",
        "- Collect performance metrics for each model\n",
        "- Test sample predictions to verify model functionality\n",
        "- Handle any training errors gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execute-training",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate all models\n",
        "for model_name in models_to_compare:\n",
        "    try:\n",
        "        results, trained_model, trained_tokenizer = train_and_evaluate_model(\n",
        "            model_name, train_dataset, val_dataset, test_dataset\n",
        "        )\n",
        "        model_results[model_name] = results\n",
        "        \n",
        "        # Test prediction\n",
        "        test_text = \"LIFESTAR Android ·à™·à≤·â®·à≠ ·ãã·åã 7000 ·â•·à≠ ·ä†·ã≤·àµ ·ä†·â†·â£ ·ãç·àµ·å•\"\n",
        "        predictions = predict_entities(test_text, trained_model, trained_tokenizer, id_to_label)\n",
        "        model_results[model_name]['sample_prediction'] = predictions\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error training {model_name}: {str(e)}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "results-display",
      "metadata": {},
      "source": [
        "## 5. Display Training Results\n",
        "\n",
        "Present a comprehensive overview of all model performance metrics in a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "display-results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = pd.DataFrame(model_results).T\n",
        "print(results_df.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualizations",
      "metadata": {},
      "source": [
        "## 6. Performance Visualizations\n",
        "\n",
        "Create comprehensive visualizations to compare models across different dimensions:\n",
        "\n",
        "- **F1 Score Comparison**: Primary accuracy metric\n",
        "- **Training Time Analysis**: Efficiency during training\n",
        "- **Model Size Comparison**: Resource requirements\n",
        "- **Precision vs Recall**: Balance between metrics\n",
        "- **Radar Chart**: Overall performance profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-visualizations",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create visualizations\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# 1. F1 Score Comparison\n",
        "plt.subplot(2, 3, 1)\n",
        "models = list(model_results.keys())\n",
        "f1_scores = [model_results[model]['test_f1'] for model in models]\n",
        "bars = plt.bar(models, f1_scores, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "plt.title('F1 Score Comparison')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xticks(rotation=45)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{f1_scores[i]:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 2. Training Time Comparison\n",
        "plt.subplot(2, 3, 2)\n",
        "training_times = [model_results[model]['training_time']/60 for model in models]  # Convert to minutes\n",
        "bars = plt.bar(models, training_times, color=['orange', 'purple', 'brown'])\n",
        "plt.title('Training Time Comparison')\n",
        "plt.ylabel('Training Time (minutes)')\n",
        "plt.xticks(rotation=45)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{training_times[i]:.1f}m', ha='center', va='bottom')\n",
        "\n",
        "# 3. Model Size Comparison\n",
        "plt.subplot(2, 3, 3)\n",
        "model_sizes = [model_results[model]['model_size_millions'] for model in models]\n",
        "bars = plt.bar(models, model_sizes, color=['red', 'blue', 'green'])\n",
        "plt.title('Model Size Comparison')\n",
        "plt.ylabel('Parameters (Millions)')\n",
        "plt.xticks(rotation=45)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "             f'{model_sizes[i]:.0f}M', ha='center', va='bottom')\n",
        "\n",
        "# 4. Precision vs Recall\n",
        "plt.subplot(2, 3, 4)\n",
        "precisions = [model_results[model]['test_precision'] for model in models]\n",
        "recalls = [model_results[model]['test_recall'] for model in models]\n",
        "colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
        "for i, model in enumerate(models):\n",
        "    plt.scatter(recalls[i], precisions[i], s=100, c=colors[i], label=model.split('/')[-1])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision vs Recall')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Overall Performance Radar Chart\n",
        "plt.subplot(2, 3, 5)\n",
        "categories = ['F1 Score', 'Precision', 'Recall', 'Speed\\n(1/time)', 'Efficiency\\n(1/size)']\n",
        "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Complete the circle\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    values = [\n",
        "        model_results[model]['test_f1'],\n",
        "        model_results[model]['test_precision'], \n",
        "        model_results[model]['test_recall'],\n",
        "        1 / (model_results[model]['training_time'] / 100),  # Normalized speed\n",
        "        1 / (model_results[model]['model_size_millions'] / 100)  # Normalized efficiency\n",
        "    ]\n",
        "    values += values[:1]  # Complete the circle\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=model.split('/')[-1])\n",
        "    ax.fill(angles, values, alpha=0.25)\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(categories)\n",
        "ax.set_ylim(0, 1)\n",
        "plt.title('Overall Model Performance', size=16, y=1.1)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "analysis-recommendations",
      "metadata": {},
      "source": [
        "## 7. Detailed Analysis and Recommendations\n",
        "\n",
        "Perform comprehensive analysis to determine the best model for different use cases and provide business-oriented recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "detailed-analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Detailed Analysis and Recommendation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED ANALYSIS AND RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find best model for different criteria\n",
        "best_f1_model = max(model_results.keys(), key=lambda x: model_results[x]['test_f1'])\n",
        "fastest_model = min(model_results.keys(), key=lambda x: model_results[x]['training_time'])\n",
        "smallest_model = min(model_results.keys(), key=lambda x: model_results[x]['model_size_millions'])\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE ANALYSIS:\")\n",
        "print(f\"‚Ä¢ Best F1 Score: {best_f1_model} ({model_results[best_f1_model]['test_f1']:.4f})\")\n",
        "print(f\"‚Ä¢ Fastest Training: {fastest_model} ({model_results[fastest_model]['training_time']/60:.1f} minutes)\")\n",
        "print(f\"‚Ä¢ Smallest Model: {smallest_model} ({model_results[smallest_model]['model_size_millions']:.0f}M parameters)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "composite-scoring",
      "metadata": {},
      "source": [
        "## 8. Composite Score Calculation\n",
        "\n",
        "Calculate a weighted composite score considering multiple factors:\n",
        "- **Accuracy (50%)**: Primary importance for NER task quality\n",
        "- **Speed (30%)**: Important for production deployment\n",
        "- **Efficiency (20%)**: Resource optimization consideration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "composite-scoring",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate composite score\n",
        "print(f\"\\nüèÜ COMPOSITE SCORE CALCULATION:\")\n",
        "composite_scores = {}\n",
        "for model in models:\n",
        "    # Normalize metrics (higher is better)\n",
        "    f1_norm = model_results[model]['test_f1']\n",
        "    speed_norm = 1 / model_results[model]['training_time'] * 1000  # Normalize\n",
        "    size_norm = 1 / model_results[model]['model_size_millions'] * 100  # Normalize\n",
        "    \n",
        "    # Weighted composite score (adjust weights based on business priorities)\n",
        "    composite_score = (0.5 * f1_norm) + (0.3 * speed_norm) + (0.2 * size_norm)\n",
        "    composite_scores[model] = composite_score\n",
        "    \n",
        "    print(f\"‚Ä¢ {model}: {composite_score:.4f}\")\n",
        "\n",
        "best_overall_model = max(composite_scores.keys(), key=lambda x: composite_scores[x])\n",
        "\n",
        "print(f\"\\nüéØ FINAL RECOMMENDATION:\")\n",
        "print(f\"Based on the composite analysis considering accuracy (50%), speed (30%), and efficiency (20%):\")\n",
        "print(f\"RECOMMENDED MODEL: {best_overall_model}\")\n",
        "print(f\"Composite Score: {composite_scores[best_overall_model]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "business-analysis",
      "metadata": {},
      "source": [
        "## 9. Business Case Analysis\n",
        "\n",
        "Provide context-specific recommendations for EthioMart's e-commerce platform and Telegram channel analysis use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "business-case",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business case analysis\n",
        "print(f\"\\nüíº BUSINESS CASE ANALYSIS:\")\n",
        "print(f\"For EthioMart's e-commerce platform:\")\n",
        "\n",
        "if best_overall_model == \"xlm-roberta-base\":\n",
        "    print(\"‚Ä¢ XLM-RoBERTa provides the best balance of accuracy and multilingual support\")\n",
        "    print(\"‚Ä¢ Suitable for production deployment with high accuracy requirements\")\n",
        "    print(\"‚Ä¢ Recommended for comprehensive entity extraction across diverse Telegram channels\")\n",
        "elif best_overall_model == \"distilbert-base-multilingual-cased\":\n",
        "    print(\"‚Ä¢ DistilBERT offers good performance with faster inference\")\n",
        "    print(\"‚Ä¢ Ideal for real-time processing of high-volume Telegram messages\")\n",
        "    print(\"‚Ä¢ Cost-effective solution for resource-constrained environments\")\n",
        "else:\n",
        "    print(\"‚Ä¢ BERT provides solid baseline performance\")\n",
        "    print(\"‚Ä¢ Good option for balanced accuracy and resource usage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-results",
      "metadata": {},
      "source": [
        "## 10. Save Analysis Results\n",
        "\n",
        "Export comprehensive results for future reference and reporting. This includes:\n",
        "- Complete model comparison metrics\n",
        "- Composite scores and rankings\n",
        "- Final recommendations\n",
        "- Timestamp for analysis reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-results-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "final_results = {\n",
        "    'model_comparison': model_results,\n",
        "    'composite_scores': composite_scores,\n",
        "    'recommendations': {\n",
        "        'best_accuracy': best_f1_model,\n",
        "        'fastest': fastest_model,\n",
        "        'most_efficient': smallest_model,\n",
        "        'best_overall': best_overall_model\n",
        "    },\n",
        "    'analysis_date': datetime.now().isoformat()\n",
        "}\n",
        "\n",
        "with open('model_comparison_results.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(final_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ Analysis complete! Results saved to 'model_comparison_results.json'\")\n",
        "print(f\"üìà Visualization saved as 'model_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This comprehensive model comparison provides data-driven insights for selecting the optimal transformer model for Amharic NER tasks. The analysis considers multiple factors including accuracy, efficiency, and business requirements to ensure the selected model meets both technical and operational needs for EthioMart's e-commerce platform.\n",
        "\n",
        "### Key Deliverables:\n",
        "- **Trained Models**: All three models trained and saved for deployment\n",
        "- **Performance Metrics**: Comprehensive evaluation across multiple dimensions\n",
        "- **Visualizations**: Clear charts for stakeholder communication\n",
        "- **Business Recommendations**: Context-specific guidance for model selection\n",
        "- **Exportable Results**: JSON format for integration with other systems"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
