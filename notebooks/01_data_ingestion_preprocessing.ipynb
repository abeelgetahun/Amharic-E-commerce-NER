{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c92933f",
   "metadata": {},
   "source": [
    "# Task 1: Data Ingestion and Preprocessing\n",
    "## Amharic E-commerce NER Project\n",
    "\n",
    "This notebook implements data collection from Ethiopian Telegram e-commerce channels and preprocessing for NER tasks.\n",
    "\n",
    "### Objectives:\n",
    "- Connect to relevant Telegram channels\n",
    "- Implement message ingestion system\n",
    "- Preprocess Amharic text data\n",
    "- Clean and structure data into unified format\n",
    "- Store preprocessed data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b0ce3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fda115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "üìÖ Execution started at: 2025-06-23 20:06:30.605528\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from data_collection.telegram_scraper import TelegramScraper\n",
    "from preprocessing.amharic_processor import AmharicTextProcessor\n",
    "from preprocessing.data_unifier import DataUnifier\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÖ Execution started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb811dfd",
   "metadata": {},
   "source": [
    "## 2. Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a5e761e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 19:40:01,103 - task1_notebook - INFO - Task 1 notebook logging initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Logging setup complete\n"
     ]
    }
   ],
   "source": [
    "def setup_notebook_logging():\n",
    "    \"\"\"Setup logging for notebook execution\"\"\"\n",
    "    # Create logs directory\n",
    "    logs_dir = Path(\"../logs\")\n",
    "    logs_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Clear any existing handlers\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler('../logs/task1_notebook.log', encoding='utf-8'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return logging.getLogger('task1_notebook')\n",
    "\n",
    "logger = setup_notebook_logging()\n",
    "logger.info(\"Task 1 notebook logging initialized\")\n",
    "print(\"üìù Logging setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33aaf87",
   "metadata": {},
   "source": [
    "## 3. Configuration and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444e2a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Telegram API credentials loaded successfully\n",
      "API ID: 279...109\n",
      "üìÅ Directory structure created\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check API credentials\n",
    "api_id = os.getenv('TELEGRAM_API_ID')\n",
    "api_hash = os.getenv('TELEGRAM_API_HASH')\n",
    "\n",
    "if not api_id or not api_hash:\n",
    "    print(\"‚ùå Error: Telegram API credentials not found!\")\n",
    "    print(\"Please set TELEGRAM_API_ID and TELEGRAM_API_HASH in your .env file\")\n",
    "else:\n",
    "    print(\"‚úÖ Telegram API credentials loaded successfully\")\n",
    "    print(f\"API ID: {api_id[:3]}...{api_id[-3:]}\")\n",
    "    \n",
    "# Create necessary directories\n",
    "directories = [\n",
    "    \"../data/raw/telegram_scrapes\",\n",
    "    \"../data/processed\",\n",
    "    \"../logs\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"üìÅ Directory structure created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f62933",
   "metadata": {},
   "source": [
    "## 4. Channel Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1a138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Target Telegram Channels:\n",
      "  1. ZemenExpress - No description\n",
      "  2. nevacomputer - No description\n",
      "  3. meneshayeofficial - No description\n",
      "  4. ethio_brand_collection - No description\n",
      "  5. Leyueqa - No description\n",
      "  6. Shewabrand - No description\n",
      "  7. helloomarketethiopia - No description\n",
      "  8. modernshoppingcenter - No description\n",
      "  9. qnashcom - No description\n",
      "  10. Fashiontera - No description\n",
      "  11. kuruwear - No description\n",
      "  12. gebeyaadama - No description\n",
      "  13. forfreemarket - No description\n",
      "  14. classybrands - No description\n",
      "  15. marakibrand - No description\n",
      "  16. aradabrand2 - No description\n",
      "  17. @marakisat2 - No description\n",
      "  18. belaclassic - No description\n",
      "  19. AwasMart - No description\n",
      "  20. qnashcom - No description\n",
      "\n",
      "‚öôÔ∏è Scraping Configuration:\n",
      "  ‚Ä¢ max_messages_per_channel: 1000\n",
      "  ‚Ä¢ date_range_days: 60\n",
      "  ‚Ä¢ include_media: True\n",
      "  ‚Ä¢ retry_attempts: 3\n",
      "  ‚Ä¢ delay_between_requests: 2\n"
     ]
    }
   ],
   "source": [
    "# Display target channels configuration\n",
    "config_path = \"../config/channels.yaml\"\n",
    "\n",
    "if Path(config_path).exists():\n",
    "    import yaml\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"üéØ Target Telegram Channels:\")\n",
    "    for i, channel in enumerate(config['telegram_channels'], 1):\n",
    "        description = channel.get('description', 'No description')\n",
    "        print(f\"  {i}. {channel['username']} - {description}\")\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Scraping Configuration:\")\n",
    "    scraping_config = config['scraping_config']\n",
    "    for key, value in scraping_config.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: channels.yaml not found. Using default configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae777db",
   "metadata": {},
   "source": [
    "## 5. Data Ingestion - Telegram Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 19:40:28,559 - task1_notebook - INFO - Starting Telegram data scraping\n",
      "2025-06-23 19:40:28,584 - telethon.network.mtprotosender - INFO - Connecting to 149.154.167.51:443/TcpFull...\n",
      "2025-06-23 19:40:28,730 - telethon.network.mtprotosender - INFO - Connection to 149.154.167.51:443/TcpFull complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Telegram...\n"
     ]
    }
   ],
   "source": [
    "async def run_telegram_scraping():\n",
    "    \"\"\"Execute Telegram scraping process\"\"\"\n",
    "    logger.info(\"Starting Telegram data scraping\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize scraper\n",
    "        scraper = TelegramScraper(api_id, api_hash, '../config/channels.yaml')\n",
    "        print(\"üîå Connecting to Telegram...\")\n",
    "        \n",
    "        # Start session\n",
    "        await scraper.start_session()\n",
    "        print(\"‚úÖ Connected to Telegram successfully\")\n",
    "        \n",
    "        # Scrape all channels\n",
    "        print(\"üöÄ Starting data collection from channels...\")\n",
    "        all_data = await scraper.scrape_all_channels()\n",
    "        \n",
    "        # Display results\n",
    "        total_messages = 0\n",
    "        print(\"\\nüìä Scraping Results:\")\n",
    "        for channel, messages in all_data.items():\n",
    "            message_count = len(messages)\n",
    "            total_messages += message_count\n",
    "            print(f\"  ‚Ä¢ {channel}: {message_count} messages\")\n",
    "        \n",
    "        print(f\"\\nüéâ Total messages collected: {total_messages}\")\n",
    "        \n",
    "        # Close session\n",
    "        await scraper.close_session()\n",
    "        print(\"üîê Telegram session closed\")\n",
    "        \n",
    "        return all_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping: {str(e)}\")\n",
    "        print(f\"‚ùå Scraping failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the scraping\n",
    "scraping_results = await run_telegram_scraping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dca1ba",
   "metadata": {},
   "source": [
    "## 6. Data Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "197a75f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scraping_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mscraping_results\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Analyze scraped data\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Analyzing scraped data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Combine all messages for analysis\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'scraping_results' is not defined"
     ]
    }
   ],
   "source": [
    "if scraping_results:\n",
    "    # Analyze scraped data\n",
    "    print(\"üîç Analyzing scraped data...\")\n",
    "    \n",
    "    # Combine all messages for analysis\n",
    "    all_messages = []\n",
    "    for channel, messages in scraping_results.items():\n",
    "        all_messages.extend(messages)\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_raw = pd.DataFrame(all_messages)\n",
    "    \n",
    "    print(f\"\\nüìà Raw Data Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total messages: {len(df_raw)}\")\n",
    "    print(f\"  ‚Ä¢ Unique channels: {df_raw['channel_username'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Messages with text: {df_raw['text'].notna().sum()}\")\n",
    "    print(f\"  ‚Ä¢ Messages with media: {df_raw['has_media'].sum()}\")\n",
    "    print(f\"  ‚Ä¢ Date range: {df_raw['date'].min()} to {df_raw['date'].max()}\")\n",
    "    \n",
    "    # Channel distribution\n",
    "    print(f\"\\nüìä Messages per channel:\")\n",
    "    channel_counts = df_raw['channel_username'].value_counts()\n",
    "    for channel, count in channel_counts.items():\n",
    "        print(f\"  ‚Ä¢ {channel}: {count} messages\")\n",
    "    \n",
    "    # Sample messages\n",
    "    print(f\"\\nüìù Sample messages:\")\n",
    "    sample_messages = df_raw['text'].dropna().head(3)\n",
    "    for i, message in enumerate(sample_messages, 1):\n",
    "        print(f\"  {i}. {message[:100]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No data to analyze - scraping may have failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634c3cd",
   "metadata": {},
   "source": [
    "## 7. Data Preprocessing and Unification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de045db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting data unification and preprocessing\n",
      "üîÑ Starting data unification and preprocessing...\n",
      "\n",
      "‚úÖ Data unification completed!\n",
      "üìä Unified dataset statistics:\n",
      "  ‚Ä¢ Total processed messages: 1403\n",
      "  ‚Ä¢ Amharic messages: 502\n",
      "  ‚Ä¢ Messages with price hints: 951\n",
      "  ‚Ä¢ Messages with location hints: 745\n",
      "  ‚Ä¢ Messages with product hints: 77\n",
      "  ‚Ä¢ Average token count: 70.09\n"
     ]
    }
   ],
   "source": [
    "def run_data_unification():\n",
    "    \"\"\"Execute data unification and preprocessing\"\"\"\n",
    "    # Use logger if available, otherwise fallback to print\n",
    "    log = logger if 'logger' in globals() else None\n",
    "    if log:\n",
    "        log.info(\"Starting data unification and preprocessing\")\n",
    "    else:\n",
    "        print(\"INFO: Starting data unification and preprocessing\")\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Starting data unification and preprocessing...\")\n",
    "        \n",
    "        # Initialize data unifier\n",
    "        unifier = DataUnifier(\"../data/raw/telegram_scrapes\")\n",
    "        \n",
    "        # Create unified dataset\n",
    "        unified_df = unifier.create_unified_dataset()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Data unification completed!\")\n",
    "        print(f\"üìä Unified dataset statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total processed messages: {len(unified_df)}\")\n",
    "        print(f\"  ‚Ä¢ Amharic messages: {unified_df['is_amharic'].sum()}\")\n",
    "        print(f\"  ‚Ä¢ Messages with price hints: {unified_df['has_price_hints'].sum()}\")\n",
    "        print(f\"  ‚Ä¢ Messages with location hints: {unified_df['has_location_hints'].sum()}\")\n",
    "        print(f\"  ‚Ä¢ Messages with product hints: {unified_df['has_product_hints'].sum()}\")\n",
    "        print(f\"  ‚Ä¢ Average token count: {unified_df['token_count'].mean():.2f}\")\n",
    "        \n",
    "        return unified_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        if log:\n",
    "            log.error(f\"Error during data unification: {str(e)}\")\n",
    "        print(f\"‚ùå Data unification failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run data unification\n",
    "unified_dataset = run_data_unification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fda91c",
   "metadata": {},
   "source": [
    "## 8. Amharic Text Processing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184e8ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Analyzing Amharic text processing...\n",
      "\n",
      "üìù Sample Amharic text processing:\n",
      "\n",
      "Original text: MAGIC REMOTE ·à∏·å†·äï ·å®·à≠·à∞·äì·àç! ·ä≠·çç·àà·àÄ·åà·à≠ ·ã´·àã·âΩ·àÅ ·ã∞·äï·â†·äû·âª·âΩ·äï·àù ·ã≠·àÑ·äï ·ä•·âÉ ·â•·àã·âΩ·àÅ ·â•·à≠ ·ä†·â≥·àµ·åà·â°.\n",
      "Tokens (14): ['MAGIC', 'REMOTE', '·à∏·å†·äï', '·å®·à≠·à∞·äì·àç', '!', '·ä≠·çç·àà·àÄ·åà·à≠', '·ã´·àã·âΩ·àÅ', '·ã∞·äï·â†·äû·âª·âΩ·äï·àù', '·ã≠·àÑ·äï', '·ä•·âÉ']...\n",
      "\n",
      "Entity hints found:\n",
      "\n",
      "üìà Entity hints distribution:\n",
      "  ‚Ä¢ Price hints: 951 messages (67.8%)\n",
      "  ‚Ä¢ Location hints: 745 messages (53.1%)\n",
      "  ‚Ä¢ Product hints: 77 messages (5.5%)\n"
     ]
    }
   ],
   "source": [
    "if unified_dataset is not None:\n",
    "    # Analyze Amharic text processing results\n",
    "    print(\"üî§ Analyzing Amharic text processing...\")\n",
    "    \n",
    "    # Initialize text processor for demonstration\n",
    "    processor = AmharicTextProcessor()\n",
    "    \n",
    "    # Sample text analysis\n",
    "    amharic_messages = unified_dataset[unified_dataset['is_amharic'] == True]\n",
    "    \n",
    "    if len(amharic_messages) > 0:\n",
    "        print(f\"\\nüìù Sample Amharic text processing:\")\n",
    "        \n",
    "        sample_text = amharic_messages['cleaned_text'].iloc[0]\n",
    "        print(f\"\\nOriginal text: {sample_text}\")\n",
    "        \n",
    "        # Tokenization\n",
    "        tokens = processor.tokenize_amharic(sample_text)\n",
    "        print(f\"Tokens ({len(tokens)}): {tokens[:10]}...\")\n",
    "        \n",
    "        # Entity hints\n",
    "        entity_hints = processor.extract_entities_hints(sample_text)\n",
    "        print(f\"\\nEntity hints found:\")\n",
    "        for entity_type, hints in entity_hints.items():\n",
    "            if hints:\n",
    "                print(f\"  ‚Ä¢ {entity_type}: {hints}\")\n",
    "    \n",
    "    # Entity distribution analysis\n",
    "    print(f\"\\nüìà Entity hints distribution:\")\n",
    "    entity_stats = {\n",
    "        'Price hints': unified_dataset['has_price_hints'].sum(),\n",
    "        'Location hints': unified_dataset['has_location_hints'].sum(),\n",
    "        'Product hints': unified_dataset['has_product_hints'].sum()\n",
    "    }\n",
    "    \n",
    "    for entity_type, count in entity_stats.items():\n",
    "        percentage = (count / len(unified_dataset)) * 100\n",
    "        print(f\"  ‚Ä¢ {entity_type}: {count} messages ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf92476d",
   "metadata": {},
   "source": [
    "## 9. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e9d851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Assessing data quality...\n",
      "\n",
      "üìä Data Quality Metrics:\n",
      "  ‚Ä¢ Total messages: 1403\n",
      "  ‚Ä¢ Amharic messages: 502\n",
      "  ‚Ä¢ Non-empty messages: 1403\n",
      "  ‚Ä¢ Messages with entities: 1034\n",
      "  ‚Ä¢ Average message length: 70.09\n",
      "  ‚Ä¢ Min message length: 3\n",
      "  ‚Ä¢ Max message length: 274\n",
      "\n",
      "üí° Quality Assessment:\n",
      "  ‚ö†Ô∏è Low Amharic content ratio: 35.8%\n",
      "  ‚úÖ Good entity coverage: 73.7%\n",
      "  ‚úÖ Good average message length\n"
     ]
    }
   ],
   "source": [
    "if unified_dataset is not None:\n",
    "    print(\"üîç Assessing data quality...\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    quality_metrics = {\n",
    "        'Total messages': len(unified_dataset),\n",
    "        'Amharic messages': unified_dataset['is_amharic'].sum(),\n",
    "        'Non-empty messages': unified_dataset['cleaned_text'].notna().sum(),\n",
    "        'Messages with entities': len(unified_dataset[\n",
    "            unified_dataset['has_price_hints'] | \n",
    "            unified_dataset['has_location_hints'] | \n",
    "            unified_dataset['has_product_hints']\n",
    "        ]),\n",
    "        'Average message length': unified_dataset['token_count'].mean(),\n",
    "        'Min message length': unified_dataset['token_count'].min(),\n",
    "        'Max message length': unified_dataset['token_count'].max()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Data Quality Metrics:\")\n",
    "    for metric, value in quality_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  ‚Ä¢ {metric}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ {metric}: {value}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüí° Quality Assessment:\")\n",
    "    amharic_ratio = unified_dataset['is_amharic'].sum() / len(unified_dataset)\n",
    "    entity_ratio = quality_metrics['Messages with entities'] / len(unified_dataset)\n",
    "    \n",
    "    if amharic_ratio > 0.7:\n",
    "        print(f\"  ‚úÖ Good Amharic content ratio: {amharic_ratio:.1%}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Low Amharic content ratio: {amharic_ratio:.1%}\")\n",
    "    \n",
    "    if entity_ratio > 0.3:\n",
    "        print(f\"  ‚úÖ Good entity coverage: {entity_ratio:.1%}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Low entity coverage: {entity_ratio:.1%}\")\n",
    "    \n",
    "    if quality_metrics['Average message length'] > 5:\n",
    "        print(f\"  ‚úÖ Good average message length\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Short average message length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d6a5d",
   "metadata": {},
   "source": [
    "## 10. Save Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80bbc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Task 1 Execution Summary:\n",
      "  ‚Ä¢ Status: COMPLETED\n",
      "  ‚Ä¢ Messages processed: 1403\n",
      "  ‚Ä¢ Summary saved to: ..\\logs\\task1_summary.json\n",
      "\n",
      "‚úÖ Task 1 completed successfully!\n",
      "üìÇ Output files:\n",
      "  ‚Ä¢ ../data/raw/telegram_scrapes/\n",
      "  ‚Ä¢ ../data/processed/unified_dataset.csv\n",
      "  ‚Ä¢ ../data/processed/unified_dataset.json\n",
      "  ‚Ä¢ ../data/processed/dataset_summary.json\n",
      "\n",
      "üöÄ Ready for Task 2: Data Labeling\n",
      "Task 1 notebook execution completed\n"
     ]
    }
   ],
   "source": [
    "# Generate final summary\n",
    "task1_summary = {\n",
    "    'execution_time': datetime.now().isoformat(),\n",
    "    'status': 'completed' if unified_dataset is not None else 'failed',\n",
    "    'data_collected': len(unified_dataset) if unified_dataset is not None else 0,\n",
    "    #'channels_processed': len(scraping_results) if scraping_results else 0,\n",
    "    'files_created': [\n",
    "        '../data/raw/telegram_scrapes/',\n",
    "        '../data/processed/unified_dataset.csv',\n",
    "        '../data/processed/unified_dataset.json',\n",
    "        '../data/processed/dataset_summary.json'\n",
    "    ],\n",
    "    'next_steps': [\n",
    "        'Review data quality',\n",
    "        'Proceed to Task 2: Data Labeling',\n",
    "        'Prepare CoNLL format dataset'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save task summary\n",
    "summary_path = Path(\"../logs/task1_summary.json\")\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(task1_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nüéâ Task 1 Execution Summary:\")\n",
    "print(f\"  ‚Ä¢ Status: {task1_summary['status'].upper()}\")\n",
    "print(f\"  ‚Ä¢ Messages processed: {task1_summary['data_collected']}\")\n",
    "#print(f\"  ‚Ä¢ Channels processed: {task1_summary['channels_processed']}\")\n",
    "print(f\"  ‚Ä¢ Summary saved to: {summary_path}\")\n",
    "\n",
    "if unified_dataset is not None:\n",
    "    print(\"\\n‚úÖ Task 1 completed successfully!\")\n",
    "    print(\"üìÇ Output files:\")\n",
    "    for file_path in task1_summary['files_created']:\n",
    "        print(f\"  ‚Ä¢ {file_path}\")\n",
    "    print(\"\\nüöÄ Ready for Task 2: Data Labeling\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Task 1 completed with errors\")\n",
    "    print(\"Please check the logs for details\")\n",
    "\n",
    "print(\"Task 1 notebook execution completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
